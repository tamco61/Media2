{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2565747,"sourceType":"datasetVersion","datasetId":1557385}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Лабораторная работа №7 (Проведение исследований моделями семантической сегментации)\n\n## 1.\tВыбор начальных условий\n## Был выбран подходящий для задачи [датасет](https://www.kaggle.com/datasets/ashish2001/semantic-segmentation-of-underwater-imagery-suim). Модель обученная на нём, может исспользоваться в подводных исследованиях: загрязнение дна водоёмов, отслеживание популяции рыб - также есть потенциал в проведение спасательных операциях под водой, поиск потерянных вещей итп. \n\n## метрики, которые будем использовать: IoU\n","metadata":{}},{"cell_type":"code","source":"!pip install segmentation-models-pytorch albumentations timm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:45:55.354416Z","iopub.execute_input":"2025-05-17T12:45:55.354780Z","iopub.status.idle":"2025-05-17T12:47:24.782296Z","shell.execute_reply.started":"2025-05-17T12:45:55.354755Z","shell.execute_reply":"2025-05-17T12:47:24.781340Z"}},"outputs":[{"name":"stdout","text":"Collecting segmentation-models-pytorch\n  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.5)\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\nRequirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.31.1)\nRequirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.26.4)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (11.1.0)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.5.3)\nRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.21.0+cu124)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (4.67.1)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.15.2)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\nRequirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.4)\nRequirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.23)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\nRequirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (3.12.3)\nRequirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (6.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation-models-pytorch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.3->segmentation-models-pytorch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.3->segmentation-models-pytorch) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.3->segmentation-models-pytorch) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.3->segmentation-models-pytorch) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.4.26)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.3->segmentation-models-pytorch) (2024.2.0)\nDownloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, segmentation-models-pytorch\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 segmentation-models-pytorch-0.5.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 2.\tСоздание бейзлайна и оценка качества\n","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport segmentation_models_pytorch as smp\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.model_selection import train_test_split\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:47:24.783899Z","iopub.execute_input":"2025-05-17T12:47:24.784207Z","iopub.status.idle":"2025-05-17T12:47:37.571739Z","shell.execute_reply.started":"2025-05-17T12:47:24.784171Z","shell.execute_reply":"2025-05-17T12:47:37.571196Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.7' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/semantic-segmentation-of-underwater-imagery-suim/train_val\"\n\nIMG_DIR = os.path.join(DATA_DIR, \"images\")\nMASK_DIR = os.path.join(DATA_DIR, \"masks\")\n\nimage_paths = sorted([os.path.join(IMG_DIR, img) for img in os.listdir(IMG_DIR)])\nmask_paths = sorted([os.path.join(MASK_DIR, mask) for mask in os.listdir(MASK_DIR)])\n\ntrain_imgs, val_imgs, train_masks, val_masks = train_test_split(image_paths, mask_paths, test_size=0.2, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:47:37.572363Z","iopub.execute_input":"2025-05-17T12:47:37.572693Z","iopub.status.idle":"2025-05-17T12:47:37.659174Z","shell.execute_reply.started":"2025-05-17T12:47:37.572676Z","shell.execute_reply":"2025-05-17T12:47:37.658640Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"SUIM_COLORS = {\n    (0, 0, 0): 0,        \n    (0, 0, 255): 1,     \n    (0, 255, 0): 2,      \n    (255, 0, 0): 3,     \n    (255, 255, 0): 4,  \n    (255, 0, 255): 5,   \n    (0, 255, 255): 6,    \n    (255, 255, 255): 7  \n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:47:37.660902Z","iopub.execute_input":"2025-05-17T12:47:37.661134Z","iopub.status.idle":"2025-05-17T12:47:37.665418Z","shell.execute_reply.started":"2025-05-17T12:47:37.661110Z","shell.execute_reply":"2025-05-17T12:47:37.664756Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def convert_mask(mask):\n    h, w, _ = mask.shape\n    new_mask = np.zeros((h, w), dtype=np.uint8)\n\n    for rgb, class_id in SUIM_COLORS.items():\n        match = np.all(mask == rgb, axis=-1)\n        new_mask[match] = class_id\n\n    return new_mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:47:37.666092Z","iopub.execute_input":"2025-05-17T12:47:37.666311Z","iopub.status.idle":"2025-05-17T12:47:37.687737Z","shell.execute_reply.started":"2025-05-17T12:47:37.666294Z","shell.execute_reply":"2025-05-17T12:47:37.687233Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class SUIMDataset(Dataset):\n    def __init__(self, image_paths, mask_paths, transform=None):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = cv2.imread(self.image_paths[idx])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n        mask = cv2.imread(self.mask_paths[idx])\n        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n        mask = convert_mask(mask) \n    \n        \n        image = cv2.resize(image, (256, 256), interpolation=cv2.INTER_LINEAR)\n        mask = cv2.resize(mask, (256, 256), interpolation=cv2.INTER_NEAREST)\n    \n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image = augmented[\"image\"]\n            mask = augmented[\"mask\"].long()\n    \n        return image, mask\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:47:37.688414Z","iopub.execute_input":"2025-05-17T12:47:37.688678Z","iopub.status.idle":"2025-05-17T12:47:37.708922Z","shell.execute_reply.started":"2025-05-17T12:47:37.688656Z","shell.execute_reply":"2025-05-17T12:47:37.708388Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"transform = A.Compose([\n    A.Resize(256, 256),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n    A.Normalize(),\n    ToTensorV2()\n])\n\ntrain_dataset = SUIMDataset(train_imgs, train_masks, transform=transform)\nval_dataset = SUIMDataset(val_imgs, val_masks, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:47:37.709468Z","iopub.execute_input":"2025-05-17T12:47:37.709683Z","iopub.status.idle":"2025-05-17T12:47:37.738173Z","shell.execute_reply.started":"2025-05-17T12:47:37.709668Z","shell.execute_reply":"2025-05-17T12:47:37.737742Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## В качестве CNN модели будем использовать связку UNET + Resnet предобученую на imagenet.","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel_resnet34 = smp.Unet(\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=8, \n).to(device)\n\nloss_fn = smp.losses.DiceLoss(mode='multiclass')\noptimizer = torch.optim.Adam(model_resnet34.parameters(), lr=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T11:57:47.196926Z","iopub.execute_input":"2025-05-17T11:57:47.197495Z","iopub.status.idle":"2025-05-17T11:57:47.611456Z","shell.execute_reply.started":"2025-05-17T11:57:47.197472Z","shell.execute_reply":"2025-05-17T11:57:47.610885Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"for epoch in range(10):\n    model.train()\n    total_loss = 0\n\n    for imgs, masks in train_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        preds = model_resnet34(imgs)\n        loss = loss_fn(preds, masks)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T11:57:51.181892Z","iopub.execute_input":"2025-05-17T11:57:51.182210Z","iopub.status.idle":"2025-05-17T12:20:54.218199Z","shell.execute_reply.started":"2025-05-17T11:57:51.182187Z","shell.execute_reply":"2025-05-17T12:20:54.217465Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.6206\nEpoch 2, Loss: 0.4639\nEpoch 3, Loss: 0.3966\nEpoch 4, Loss: 0.3676\nEpoch 5, Loss: 0.3504\nEpoch 6, Loss: 0.3282\nEpoch 7, Loss: 0.3119\nEpoch 8, Loss: 0.3141\nEpoch 9, Loss: 0.2907\nEpoch 10, Loss: 0.2762\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def multiclass_iou_score(preds, targets, num_classes=8, eps=1e-6):\n    ious = []\n    preds = preds.argmax(dim=1)\n\n    preds = preds.cpu().numpy() \n    targets = targets.cpu().numpy()\n\n    for cls in range(num_classes):\n        pred_inds = (preds == cls)\n        target_inds = (targets == cls)\n\n        intersection = (pred_inds & target_inds).sum()\n        union = (pred_inds | target_inds).sum()\n\n        if union == 0:\n            continue\n\n        ious.append((intersection + eps) / (union + eps))\n\n    return np.nanmean(ious)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:22:41.574211Z","iopub.execute_input":"2025-05-17T12:22:41.574482Z","iopub.status.idle":"2025-05-17T12:22:41.579607Z","shell.execute_reply.started":"2025-05-17T12:22:41.574463Z","shell.execute_reply":"2025-05-17T12:22:41.578980Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"model_resnet34.eval()\nious = []\n\nwith torch.no_grad():\n    for imgs, masks in val_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        preds = model_resnet34(imgs)\n        iou = multiclass_iou_score(preds, masks)\n        ious.append(iou)\n\nprint(\"Mean IoU:\", np.mean(ious))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:22:43.629348Z","iopub.execute_input":"2025-05-17T12:22:43.629912Z","iopub.status.idle":"2025-05-17T12:23:20.487544Z","shell.execute_reply.started":"2025-05-17T12:22:43.629889Z","shell.execute_reply":"2025-05-17T12:23:20.486886Z"}},"outputs":[{"name":"stdout","text":"Mean IoU: 0.4414588024476484\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## В качестве трансформенной модели у нас будет UNET + MiT предобученую на imagenet.","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel_mit0 = smp.Unet(\n    encoder_name=\"mit_b0\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=8,\n).to(device)\n\n\n\nloss_fn = smp.losses.DiceLoss(mode='multiclass')\noptimizer = torch.optim.Adam(model_mit0.parameters(), lr=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:52:15.121444Z","iopub.execute_input":"2025-05-17T12:52:15.121739Z","iopub.status.idle":"2025-05-17T12:52:15.292875Z","shell.execute_reply.started":"2025-05-17T12:52:15.121717Z","shell.execute_reply":"2025-05-17T12:52:15.292307Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"for epoch in range(10):\n    model_mit0.train()\n    total_loss = 0\n\n    for imgs, masks in train_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        preds = model_mit0(imgs)\n        loss = loss_fn(preds, masks)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:52:50.876889Z","iopub.execute_input":"2025-05-17T12:52:50.877447Z","iopub.status.idle":"2025-05-17T13:15:51.031670Z","shell.execute_reply.started":"2025-05-17T12:52:50.877422Z","shell.execute_reply":"2025-05-17T13:15:51.030794Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.6338\nEpoch 2, Loss: 0.4705\nEpoch 3, Loss: 0.3929\nEpoch 4, Loss: 0.3544\nEpoch 5, Loss: 0.3248\nEpoch 6, Loss: 0.3087\nEpoch 7, Loss: 0.3073\nEpoch 8, Loss: 0.2782\nEpoch 9, Loss: 0.2497\nEpoch 10, Loss: 0.2423\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def multiclass_iou_score(preds, targets, num_classes=8, eps=1e-6):\n    ious = []\n    preds = preds.argmax(dim=1) \n\n    preds = preds.cpu().numpy()\n    targets = targets.cpu().numpy()\n\n    for cls in range(num_classes):\n        pred_inds = (preds == cls)\n        target_inds = (targets == cls)\n\n        intersection = (pred_inds & target_inds).sum()\n        union = (pred_inds | target_inds).sum()\n\n        if union == 0:\n            continue\n\n        ious.append((intersection + eps) / (union + eps))\n\n    return np.nanmean(ious)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T13:15:51.033201Z","iopub.execute_input":"2025-05-17T13:15:51.033406Z","iopub.status.idle":"2025-05-17T13:15:51.038423Z","shell.execute_reply.started":"2025-05-17T13:15:51.033391Z","shell.execute_reply":"2025-05-17T13:15:51.037853Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"model_mit0.eval()\nious = []\n\nwith torch.no_grad():\n    for imgs, masks in val_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        preds = model_mit0(imgs)\n        iou = multiclass_iou_score(preds, masks)\n        ious.append(iou)\n\nprint(\"Mean IoU:\", np.mean(ious))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T13:17:21.174638Z","iopub.execute_input":"2025-05-17T13:17:21.175141Z","iopub.status.idle":"2025-05-17T13:18:01.598495Z","shell.execute_reply.started":"2025-05-17T13:17:21.175120Z","shell.execute_reply":"2025-05-17T13:18:01.597862Z"}},"outputs":[{"name":"stdout","text":"Mean IoU: 0.45577886832064907\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## По лоссу и метрики качества нетрудно заметить, что трансформенная модель показывает лучшие результаты по сранению с CNN. Учитывая разницу между resnet34 и mit_b0, а именно, в размере, который по скромным оценкам меньше в 2 раза, то можно сделать вывод, что MIT вне конкуренции в этой задаче при наших вводных.","metadata":{}},{"cell_type":"markdown","source":"# 3.\tУлучшение бейзлайна\n## Гипотеза: увеличим количество эпох и возьмём старшие модели.","metadata":{}},{"cell_type":"markdown","source":"## RESNET50","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel_resnet50 = smp.Unet(\n    encoder_name=\"resnet50\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=8,\n).to(device)\n\nloss_fn = smp.losses.DiceLoss(mode='multiclass')\noptimizer = torch.optim.Adam(model_resnet50.parameters(), lr=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T13:19:26.706112Z","iopub.execute_input":"2025-05-17T13:19:26.706362Z","iopub.status.idle":"2025-05-17T13:19:28.048858Z","shell.execute_reply.started":"2025-05-17T13:19:26.706346Z","shell.execute_reply":"2025-05-17T13:19:28.048176Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd7f87b421bb4b0a802fd14202a9dc8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff1e6c8462ec4410aef8677a7e8f788b"}},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"for epoch in range(15):\n    model_resnet50.train()\n    total_loss = 0\n\n    for imgs, masks in train_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        preds = model_resnet50(imgs)\n        loss = loss_fn(preds, masks)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T13:24:41.309755Z","iopub.execute_input":"2025-05-17T13:24:41.309993Z","iopub.status.idle":"2025-05-17T14:01:57.704618Z","shell.execute_reply.started":"2025-05-17T13:24:41.309977Z","shell.execute_reply":"2025-05-17T14:01:57.703810Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.3982\nEpoch 2, Loss: 0.3517\nEpoch 3, Loss: 0.3341\nEpoch 4, Loss: 0.3239\nEpoch 5, Loss: 0.2999\nEpoch 6, Loss: 0.2819\nEpoch 7, Loss: 0.2717\nEpoch 8, Loss: 0.2642\nEpoch 9, Loss: 0.2507\nEpoch 10, Loss: 0.2410\nEpoch 11, Loss: 0.2437\nEpoch 12, Loss: 0.2400\nEpoch 13, Loss: 0.2299\nEpoch 14, Loss: 0.2477\nEpoch 15, Loss: 0.2302\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"model_resnet50.eval()\nious = []\n\nwith torch.no_grad():\n    for imgs, masks in val_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        preds = model_resnet50(imgs)\n        iou = multiclass_iou_score(preds, masks)\n        ious.append(iou)\n\nprint(\"Mean IoU:\", np.mean(ious))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:01:57.706112Z","iopub.execute_input":"2025-05-17T14:01:57.706334Z","iopub.status.idle":"2025-05-17T14:02:29.650764Z","shell.execute_reply.started":"2025-05-17T14:01:57.706317Z","shell.execute_reply":"2025-05-17T14:02:29.650127Z"}},"outputs":[{"name":"stdout","text":"Mean IoU: 0.47541707360553875\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"## MIT_B1","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel_mit1 = smp.Unet(\n    encoder_name=\"mit_b1\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=8,  # SUIM: 8 классов\n).to(device)\n\nloss_fn = smp.losses.DiceLoss(mode='multiclass')\noptimizer = torch.optim.Adam(model_mit1.parameters(), lr=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:02:29.651455Z","iopub.execute_input":"2025-05-17T14:02:29.651744Z","iopub.status.idle":"2025-05-17T14:02:30.943982Z","shell.execute_reply.started":"2025-05-17T14:02:29.651716Z","shell.execute_reply":"2025-05-17T14:02:30.943219Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/135 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f706a09cc6514b5da263a7938aaf08a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/54.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b742ebdecaa54e4b89495d0c6a21c6dc"}},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"for epoch in range(15):\n    model_mit1.train()\n    total_loss = 0\n\n    for imgs, masks in train_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        preds = model_mit1(imgs)\n        loss = loss_fn(preds, masks)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:02:30.945547Z","iopub.execute_input":"2025-05-17T14:02:30.945771Z","iopub.status.idle":"2025-05-17T14:37:38.820578Z","shell.execute_reply.started":"2025-05-17T14:02:30.945756Z","shell.execute_reply":"2025-05-17T14:37:38.819574Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.5932\nEpoch 2, Loss: 0.4386\nEpoch 3, Loss: 0.3834\nEpoch 4, Loss: 0.3343\nEpoch 5, Loss: 0.3250\nEpoch 6, Loss: 0.3140\nEpoch 7, Loss: 0.2826\nEpoch 8, Loss: 0.2715\nEpoch 9, Loss: 0.2487\nEpoch 10, Loss: 0.2317\nEpoch 11, Loss: 0.2292\nEpoch 12, Loss: 0.2154\nEpoch 13, Loss: 0.2055\nEpoch 14, Loss: 0.1997\nEpoch 15, Loss: 0.1845\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"model_mit1.eval()\nious = []\n\nwith torch.no_grad():\n    for imgs, masks in val_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        preds = model_mit1(imgs)\n        iou = multiclass_iou_score(preds, masks)\n        ious.append(iou)\n\nprint(\"Mean IoU:\", np.mean(ious))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:37:38.821415Z","iopub.execute_input":"2025-05-17T14:37:38.821678Z","iopub.status.idle":"2025-05-17T14:38:10.180528Z","shell.execute_reply.started":"2025-05-17T14:37:38.821660Z","shell.execute_reply":"2025-05-17T14:38:10.179756Z"}},"outputs":[{"name":"stdout","text":"Mean IoU: 0.49538002662882064\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## Обе модели показали аналогичный рост точности по задаче, по сравнению с прошлыми результатами, что подтверждает нашу гипотезу. Опять же преимущество на стороне MIT архитектуры выводы в этом соперничестве аналогичны прошлым.","metadata":{}},{"cell_type":"markdown","source":"# 4.\tИмплементация алгоритма машинного обучения ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            \n            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels=3, num_classes=8, features=[64, 128, 256, 512]):\n        super().__init__()\n        \n        self.downs = nn.ModuleList()\n        self.ups = nn.ModuleList()\n        \n        # Downsampling\n        for feature in features:\n            self.downs.append(ConvBlock(in_channels, feature))\n            in_channels = feature\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        # Bottleneck\n        self.bottleneck = ConvBlock(features[-1], features[-1] * 2)\n        \n        # Upsampling\n        for feature in reversed(features):\n            self.ups.append(\n                nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2)\n            )\n            self.ups.append(ConvBlock(feature * 2, feature))  \n\n        # Final classifier\n        self.final_conv = nn.Conv2d(features[0], num_classes, kernel_size=1)\n\n    def forward(self, x):\n        skip_connections = []\n\n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n        \n        x = self.bottleneck(x)\n        skip_connections = skip_connections[::-1]\n        \n        for idx in range(0, len(self.ups), 2):\n            x = self.ups[idx](x)  \n            skip = skip_connections[idx // 2]\n            \n            if x.shape != skip.shape:\n                x = F.interpolate(x, size=skip.shape[2:])\n\n            x = torch.cat((skip, x), dim=1)\n            x = self.ups[idx + 1](x) \n\n        return self.final_conv(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T15:17:38.823546Z","iopub.execute_input":"2025-05-17T15:17:38.824781Z","iopub.status.idle":"2025-05-17T15:17:38.837419Z","shell.execute_reply.started":"2025-05-17T15:17:38.824694Z","shell.execute_reply":"2025-05-17T15:17:38.836654Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = UNet(in_channels=3, num_classes=8).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T15:21:35.161751Z","iopub.execute_input":"2025-05-17T15:21:35.162292Z","iopub.status.idle":"2025-05-17T15:21:35.433061Z","shell.execute_reply.started":"2025-05-17T15:21:35.162271Z","shell.execute_reply":"2025-05-17T15:21:35.432489Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"for epoch in range(10):\n    model.train()\n    total_loss = 0\n\n    for imgs, masks in train_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        preds = model(imgs)\n        loss = loss_fn(preds, masks)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T15:21:37.469052Z","iopub.execute_input":"2025-05-17T15:21:37.469322Z","iopub.status.idle":"2025-05-17T15:56:18.269568Z","shell.execute_reply.started":"2025-05-17T15:21:37.469302Z","shell.execute_reply":"2025-05-17T15:56:18.268869Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.7587\nEpoch 2, Loss: 0.7715\nEpoch 3, Loss: 0.7601\nEpoch 4, Loss: 0.7682\nEpoch 5, Loss: 0.7497\nEpoch 6, Loss: 0.7523\nEpoch 7, Loss: 0.7687\nEpoch 8, Loss: 0.7550\nEpoch 9, Loss: 0.7610\nEpoch 10, Loss: 0.7691\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"model.eval()\nious = []\n\nwith torch.no_grad():\n    for imgs, masks in val_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        preds = model(imgs)\n        iou = multiclass_iou_score(preds, masks)\n        ious.append(iou)\n\nprint(\"Mean IoU:\", np.mean(ious))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T15:56:18.270961Z","iopub.execute_input":"2025-05-17T15:56:18.271177Z","iopub.status.idle":"2025-05-17T15:56:55.420737Z","shell.execute_reply.started":"2025-05-17T15:56:18.271159Z","shell.execute_reply":"2025-05-17T15:56:55.420012Z"}},"outputs":[{"name":"stdout","text":"Mean IoU: 0.03634133792457782\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimpleConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n    def forward(self, x):\n        return self.conv(x)\n\nclass SimpleUNet(nn.Module):\n    def __init__(self, in_channels=3, num_classes=8, features=[32, 64, 128]):\n        super().__init__()\n        self.downs = nn.ModuleList()\n        self.ups = nn.ModuleList()\n        self.pool = nn.MaxPool2d(2)\n\n        for feature in features:\n            self.downs.append(SimpleConvBlock(in_channels, feature))\n            in_channels = feature\n\n        self.bottleneck = SimpleConvBlock(features[-1], features[-1] * 2)\n\n        for feature in reversed(features):\n            self.ups.append(nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2))\n            self.ups.append(SimpleConvBlock(feature * 2, feature))\n\n        self.final_conv = nn.Conv2d(features[0], num_classes, kernel_size=1)\n\n    def forward(self, x):\n        skip_connections = []\n\n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n\n        x = self.bottleneck(x)\n        skip_connections = skip_connections[::-1]\n\n        for idx in range(0, len(self.ups), 2):\n            x = self.ups[idx](x)\n            skip = skip_connections[idx // 2]\n\n            if x.shape != skip.shape:\n                x = F.interpolate(x, size=skip.shape[2:])\n\n            x = torch.cat((skip, x), dim=1)\n            x = self.ups[idx + 1](x)\n\n        return self.final_conv(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T15:56:55.421469Z","iopub.execute_input":"2025-05-17T15:56:55.421765Z","iopub.status.idle":"2025-05-17T15:56:55.431381Z","shell.execute_reply.started":"2025-05-17T15:56:55.421742Z","shell.execute_reply":"2025-05-17T15:56:55.430534Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_lite = SimpleUNet(in_channels=3, num_classes=8).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T15:56:55.432885Z","iopub.execute_input":"2025-05-17T15:56:55.433067Z","iopub.status.idle":"2025-05-17T15:56:55.471340Z","shell.execute_reply.started":"2025-05-17T15:56:55.433052Z","shell.execute_reply":"2025-05-17T15:56:55.470880Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"for epoch in range(50):\n    model_lite.train()\n    total_loss = 0\n\n    for imgs, masks in train_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        preds = model_lite(imgs)\n        loss = loss_fn(preds, masks)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T15:56:55.471982Z","iopub.execute_input":"2025-05-17T15:56:55.472202Z","iopub.status.idle":"2025-05-17T16:30:21.733496Z","shell.execute_reply.started":"2025-05-17T15:56:55.472178Z","shell.execute_reply":"2025-05-17T16:30:21.732486Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.7651\nEpoch 2, Loss: 0.7634\nEpoch 3, Loss: 0.7500\nEpoch 4, Loss: 0.7563\nEpoch 5, Loss: 0.7642\nEpoch 6, Loss: 0.7493\nEpoch 7, Loss: 0.7677\nEpoch 8, Loss: 0.7615\nEpoch 9, Loss: 0.7694\nEpoch 10, Loss: 0.7510\nEpoch 11, Loss: 0.7592\nEpoch 12, Loss: 0.7614\nEpoch 13, Loss: 0.7592\nEpoch 14, Loss: 0.7638\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/218133765.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_lite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/1119811908.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# конвертируем в классы 0-7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Приведение к одному размеру (например, 256x256)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/1971336568.py\u001b[0m in \u001b[0;36mconvert_mask\u001b[0;34m(mask)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSUIM_COLORS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mnew_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mall\u001b[0;34m(a, axis, out, keepdims, where)\u001b[0m\n\u001b[1;32m   2502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2503\u001b[0m     \"\"\"\n\u001b[0;32m-> 2504\u001b[0;31m     return _wrapreduction(a, np.logical_and, 'all', axis, None, out,\n\u001b[0m\u001b[1;32m   2505\u001b[0m                           keepdims=keepdims, where=where)\n\u001b[1;32m   2506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":38},{"cell_type":"code","source":"model_lite.eval()\nious = []\n\nwith torch.no_grad():\n    for imgs, masks in val_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        preds = model_lite(imgs)\n        iou = multiclass_iou_score(preds, masks)\n        ious.append(iou)\n\nprint(\"Mean IoU:\", np.mean(ious))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:32:15.478108Z","iopub.execute_input":"2025-05-17T16:32:15.478398Z","iopub.status.idle":"2025-05-17T16:32:47.060175Z","shell.execute_reply.started":"2025-05-17T16:32:15.478373Z","shell.execute_reply":"2025-05-17T16:32:47.059395Z"}},"outputs":[{"name":"stdout","text":"Mean IoU: 0.028478867086896457\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"## По лоссу видно что модели крайне неохотно хотят обучаться в тех же условиях что и готовые, но даже в случае хорошего обучения модели бы не показали тех же результатов, потому что готовые модели используют предобученные веса на imagenet.","metadata":{}}]}